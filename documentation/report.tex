\documentclass[11pt]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}

% Title and author format similar to NIPS-2012
\title{\textbf{\Large Vehicle Re-Identification for Origin-Destination Flow
        Estimation:} \\
    \large Bengaluru Mobility Challenge - Phase II}

\author{
    Abhijith Ganesh
    Prenitha Rajesh
    Pardheev Krishna
}

\date{September 2024}

\begin{document}

\maketitle
\begin{abstract}
    This report presents our approach for vehicle re-identification in the
    context of the Bengaluru Mobility Challenge (BMC) - Phase II. The objective
    is to re-identify vehicles seen at one location in another part of the
    network
    and estimate origin-destination (O-D) flows over a specific time period.
    Our
    solution integrates YOLOv10 object detection, OSNet for feature extraction,
    and
    Chroma for efficient vector storage. Using a custom dataset of Indian
    vehicles,
    our system achieved competitive performance in re-identification across
    multiple
    camera feeds. This document details our methodology, results, and
    conclusions.
\end{abstract}

\section{Introduction}
The task of vehicle re-identification is essential for estimating
origin-destination (O-D) flows, which are crucial for transportation planning
and traffic management. The Bengaluru Mobility Challenge (BMC) Phase II
evaluates the ability to re-identify vehicles across different locations within
a network over a specific time period. Accurate O-D flow estimation is a key
factor for effective traffic control and infrastructure development.

In this challenge, participants are tasked with re-identifying vehicles
detected by multiple camera feeds, which helps understand traffic patterns,
congestion, and vehicle movement between key points. Our approach leverages
state-of-the-art detection and feature extraction techniques to enhance the
accuracy of re-identification and prediction of O-D flows, improving the
overall traffic management system in a dynamic city like Bengaluru.

\section{Methodology}
Our solution comprises three primary stages: detection, feature extraction, and
matching.

\subsection{Detection}
We employ the YOLOv10 object detection framework \cite{wang2024yolov10} to
identify vehicles in video streams. YOLOv10 is known for its real-time
detection capability and balance between speed and accuracy.

\subsection{Feature Extraction and Re-Identification}
For the vehicle re-identification (ReID) task, we utilize the
\texttt{torchreid} library
\cite{torchreid}, which provides state-of-the-art models and tools for deep
learning-based ReID in PyTorch. Specifically, we employ the OSNet architecture,
particularly the \texttt{osnet\_x0\_75} variant, for feature extraction. OSNet
is
designed for person and object re-identification tasks, and its omni-scale
feature
learning capability makes it particularly suitable for learning discriminative
yet generalizable feature representations across various scales and conditions
\cite{zhou2019osnet, zhou2021osnet}.

OSNet's architecture enables robust ReID in complex environments with
challenging variations, such as lighting and occlusions. The model combines
information from both small and large receptive fields, ensuring that global
and local features are well-represented. This is critical for vehicle ReID,
where subtle differences between vehicles must be captured to achieve high
matching accuracy.

For feature extraction, each vehicle is passed through the OSNet model, and the
resulting feature embeddings are stored in the Chroma vector database.
Matching between different camera feeds is performed using cosine similarity
between the
extracted feature vectors. A threshold is applied to determine whether the
vehicles in different frames belong to the same identity.

\subsection{Vector Storage with Chroma}
Chroma, a vector database optimized for high-dimensional embeddings, is used to
store the vehicle feature vectors. This ensures efficient similarity searches
between frames captured by different cameras.

\subsection{Matching}
We compute cosine similarity between embeddings to match vehicles across
multiple frames and locations. A threshold is applied to determine whether the
vehicles in different frames are likely to be the same.

\section{Training Metrics}
We evaluated our solution on a dataset of 24,500 images of Indian vehicles,
divided into 19,600 training images and 4,900 testing images. The performance
of the models is summarized in Appendix (see Table
\ref{table:performance_results}).

\section{Software and Libraries}
Our project uses the following libraries:
\begin{itemize}
    \item \texttt{json} - for handling data formats.
    \item \texttt{logging} - for logging program events.
    \item \texttt{os} - to interact with the operating system.
    \item \texttt{subprocess} - for running external commands.
    \item \texttt{sys} - to manipulate runtime environment.
    \item \texttt{time} - for time-related functions.
    \item \texttt{chromadb} - for storing feature vectors.
    \item \texttt{cv2} - for image processing.
    \item \texttt{h5py} - for handling large datasets.
    \item \texttt{numpy} - for numerical operations.
    \item \texttt{torch} - for deep learning and neural networks.
    \item \texttt{comet\_ml} - for tracking model metrics.
    \item \texttt{PIL} - for image manipulation.
    \item \texttt{torchvision} - for computer vision tasks.
    \item \texttt{ultralytics} - for YOLO object detection.
    \item \textbf{PrenAbhi} - Custom library for the competition guidelines,
          data handling, and submission.
\end{itemize}

\section{Conclusions}
Our vehicle re-identification system demonstrated strong performance across
various model configurations. The combination of YOLOv10 for detection, OSNet
for feature extraction, and Chroma for vector storage allowed for accurate O-D
flow estimation. Future work includes improving detection in challenging
environments and optimizing the matching process for larger datasets. The
authors of this paper believe that this model is foundational but requires
large-scale, localized data to capture finer details in the images. Currently,
the model performs well for commercial vehicles but needs improvement for
identifying three-wheelers, bicycles, and other non-commercial vehicles.

\bibliographystyle{ieeetr}
\begin{thebibliography}{9}
    \bibitem{aicity} AI City Challenge, \url{https://www.aicitychallenge.org/}
    \bibitem{zheng2021} Z. Zheng et al., ``Connecting Language and Vision for
    Natural Language-Based Vehicle Retrieval,'' \emph{arXiv preprint}, 2021.
    Available at: \url{https://arxiv.org/pdf/2105.14897}
    \bibitem{wang2024yolov10} A. Wang et al., ``YOLOv10: Real-Time End-to-End
    Object Detection,'' \emph{arXiv preprint}, 2024.
    \bibitem{torchreid} K. Zhou and T. Xiang, ``Torchreid: A Library for Deep
    Learning Person Re-Identification in Pytorch,'' \emph{arXiv preprint
        arXiv:1910.10093}, 2019.
    \bibitem{zhou2019osnet} K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang,
    ``Omni-Scale Feature Learning for Person Re-Identification,'' in Proceedings of
    the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.
    \bibitem{zhou2021osnet} K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang,
    ``Learning Generalisable Omni-Scale Representations for Person
    Re-Identification,'' \emph{IEEE Transactions on Pattern Analysis and Machine
        Intelligence (TPAMI)}, 2021.
    \bibitem{naphade2020ai} M. Naphade et al., ``The 5th AI City Challenge,''
    \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
        Workshops (CVPRW)}, 2020.
    \bibitem{he2016resnet} K. He, X. Zhang, S. Ren, and J. Sun, ``Deep Residual
    Learning for Image Recognition,'' in Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778.
\end{thebibliography}

\appendix
\section{Appendix}
Here we provide detailed performance metrics for the models and configurations
tested.

\begin{table*}[t]
    \centering
    \caption{Comprehensive Results: Epochs, Loss, Model Parameters, Speed,
        GFLOPs, and mAP Across All Models}
    \label{table:comprehensive_results}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule
            Name              & Epochs     & Val/Cls\_Loss &
            Model/Parameters  & Batch Size & Speed (ms)    & Train/Box\_Loss &
            Model/GFLOPs      & mAP50(B)
            \\
            \midrule
            prenag-xl         & 100        & 0.748         & 31,668,362
                              & 16         & 3.907         & 1.132           &
            171.062           &
            0.944
            \\
            abhipren-aug      & 30         & 0.481         & 8,071,770
                              & 16         & 1.876         & 0.878           &
            24.796            &
            0.981
            \\
            abhipren-noaug    & 30         & 0.394         & 8,071,770
                              & 16         & 1.748         & 0.613           &
            24.796            &
            0.984
            \\
            prentrains-nonhyp & 25         & 0.394         & 8,071,770
                              & 28         & 1.019         & 0.611           &
            24.796            &
            0.985
            \\
            prentrains-hyp    & 28         & 0.425         & 8,071,770
                              & -1         & 0.966         & 0.681           &
            24.796            &
            0.980
            \\
            \bottomrule
        \end{tabular}
    }
\end{table*}

\begin{table}[h!]
    \centering
    \caption{Model Performance Results}
    \label{table:performance_results}
    \begin{tabular}{lcccc}
        \toprule
        Model              & mAP50(B) & Speed (ms) & GFLOPs  & Val/Cls\_Loss \\
        \midrule
        prenag-xl          & 0.944    & 3.907      & 171.062 & 0.748         \\
        abhipren-aug       & 0.981    & 1.876      & 24.796  & 0.481         \\
        abhipren-noaug     & 0.984    & 1.748      & 24.796  & 0.394         \\
        prentrains-nonhyp  & 0.985    & 1.019      & 24.796  & 0.394         \\
        prentrains-hyp\footnote{Batch size set to -1 was auto-optimized by the
        Ultralytics code.} & 0.980    & 0.966      & 24.796  & 0.425         \\
        \bottomrule
    \end{tabular}
\end{table}

\end{document}
