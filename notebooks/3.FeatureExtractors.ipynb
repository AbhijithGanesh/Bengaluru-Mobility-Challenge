{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "class BaseFeatureExtraction:\n",
    "    def __init__(self, img: np.ndarray, num_classes: int = 768, onnx_model_path: str = None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = None\n",
    "        self.num_classes = num_classes\n",
    "        self.img = torch.from_numpy(img).permute(2, 0, 1).float().to(self.device)\n",
    "        self.use_onnx = False\n",
    "\n",
    "        if onnx_model_path:\n",
    "            self.use_onnx = True\n",
    "            self.onnx_model_path = onnx_model_path\n",
    "\n",
    "    def process_bounding_box(self, bbox):\n",
    "        if isinstance(bbox[0], list):\n",
    "            bbox = bbox[0]\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        return self.img[:, y1:y2, x1:x2]\n",
    "\n",
    "    def preprocess_image(self, cropped_img, size=224):\n",
    "        preprocess = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size),\n",
    "                transforms.CenterCrop(size),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return preprocess(cropped_img)\n",
    "\n",
    "    def extract_features(self, bbox, size=224):\n",
    "        cropped_img = self.process_bounding_box(bbox)\n",
    "        input_tensor = (\n",
    "            self.preprocess_image(cropped_img, size=size).unsqueeze(0).to(self.device)\n",
    "        )\n",
    "\n",
    "        if self.use_onnx:\n",
    "            return self.run_onnx_inference(input_tensor)[0]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_tensor)[0]\n",
    "            return output\n",
    "    \n",
    "    def export_to_onnx(self):\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)  # Adjust size as necessary\n",
    "        torch.onnx.export(self.model, dummy_input, self.onnx_model_path, opset_version=11)\n",
    "        print(f\"Model exported to {self.onnx_model_path}\")\n",
    "\n",
    "    def load_onnx_model(self):\n",
    "        ort_session = ort.InferenceSession(self.onnx_model_path, providers=['CUDAExecutionProvider'] if self.device.type == 'cuda' else ['CPUExecutionProvider'])\n",
    "        return ort_session\n",
    "    \n",
    "    def run_onnx_inference(self, input_tensor):\n",
    "        if not hasattr(self, 'ort_session'):\n",
    "            self.ort_session = self.load_onnx_model()\n",
    "            \n",
    "        ort_inputs = {self.ort_session.get_inputs()[0].name: input_tensor.cpu().numpy()}\n",
    "        ort_outs = self.ort_session.run(None, ort_inputs)\n",
    "        return ort_outs\n",
    "\n",
    "class EfficientNetFeatureExtraction(BaseFeatureExtraction):\n",
    "    def __init__(self, img: np.ndarray, num_classes: int = 768):\n",
    "        super().__init__(img, num_classes)\n",
    "        from torchvision import models\n",
    "\n",
    "        self.model = models.efficientnet_b0(\n",
    "            weights=models.EfficientNet_B0_Weights.DEFAULT\n",
    "        )\n",
    "        self.model.classifier[1] = torch.nn.Linear(\n",
    "            self.model.classifier[1].in_features, num_classes\n",
    "        )\n",
    "        self.model.eval().to(self.device)\n",
    "\n",
    "\n",
    "class MobileNetSmallFeatureExtraction(BaseFeatureExtraction):\n",
    "    def __init__(self, img: np.ndarray, num_classes: int = 512):\n",
    "        super().__init__(img, num_classes)\n",
    "        from torchvision import models\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(\n",
    "            weights=models.MobileNet_V3_Small_Weights.DEFAULT\n",
    "        )\n",
    "        self.model.classifier[3] = torch.nn.Linear(\n",
    "            self.model.classifier[3].in_features, num_classes\n",
    "        )\n",
    "        self.model.eval().to(self.device)\n",
    "\n",
    "\n",
    "class SqueezeNetFeatureExtraction(BaseFeatureExtraction):\n",
    "    def __init__(self, img: np.ndarray, num_classes: int = 768, onnx_model_path: str = None):\n",
    "        super().__init__(img, num_classes, onnx_model_path)\n",
    "        from torchvision import models\n",
    "\n",
    "        self.model = models.squeezenet1_0(weights=models.SqueezeNet1_0_Weights.DEFAULT)\n",
    "        self.model.classifier[1] = torch.nn.Conv2d(\n",
    "            self.model.classifier[1].in_channels, num_classes, kernel_size=(1, 1)\n",
    "        )\n",
    "        self.model.eval().to(self.device)\n",
    "\n",
    "\n",
    "class InceptionFeatureExtraction(BaseFeatureExtraction):\n",
    "    def __init__(self, img: np.ndarray, num_classes: int = 768):\n",
    "        super().__init__(img, num_classes)\n",
    "        from torchvision import models\n",
    "\n",
    "        self.model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.model.eval().to(self.device)\n",
    "\n",
    "    def extract_features(self, bbox, size=299):\n",
    "        return super().extract_features(bbox, size=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"/home/abhijithganesh/bmc/bmc24/car.jpg\")\n",
    "model = SqueezeNetFeatureExtraction(img, onnx_model_path=\"squeezenet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.extract_features([[0, 0, 100, 100]])[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
