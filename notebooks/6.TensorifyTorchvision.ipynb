{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9068930,"sourceType":"datasetVersion","datasetId":5440379}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install torch tqdm torchvision av ipywidgets imageio[ffmpeg] numpy pillow","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:52:14.910674Z","iopub.execute_input":"2024-07-30T18:52:14.911055Z","iopub.status.idle":"2024-07-30T18:52:24.095875Z","shell.execute_reply.started":"2024-07-30T18:52:14.911028Z","shell.execute_reply":"2024-07-30T18:52:24.094599Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom torchvision.io import read_video, read_video_timestamps\nimport os\nfrom tqdm import tqdm\nfrom PIL import Image\nimport io\nimport zlib\nimport os\nimport torch\nfrom torchvision import transforms\nfrom torchvision.io import read_video, read_video_timestamps\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:52:24.097652Z","iopub.execute_input":"2024-07-30T18:52:24.098024Z","iopub.status.idle":"2024-07-30T18:52:45.902634Z","shell.execute_reply.started":"2024-07-30T18:52:24.097995Z","shell.execute_reply":"2024-07-30T18:52:45.901827Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def permute_tensor(tensor):\n    return tensor.permute(2, 0, 1)\n\ndef extract_and_store_frames(video_path, tensor_output_folder, frame_rate=30, chunk_size=1000):\n    \"\"\"\n    Extract frames from a video, convert them to tensors, and store them with frame IDs.\n    Args:\n    - video_path (str): Path to the video file.\n    - tensor_output_folder (str): Folder to save the tensor files.\n    - frame_rate (int): Interval of frames to be extracted (e.g., every 30th frame).\n    - chunk_size (int): Number of frames to process at a time.\n    \"\"\"\n    if not os.path.exists(tensor_output_folder):\n        os.makedirs(tensor_output_folder)\n    \n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Get video metadata\n    pts, _ = read_video_timestamps(video_path, pts_unit='sec')\n    total_frames = len(pts)\n    \n    # Define transformation\n    transform = transforms.Compose([\n        transforms.Lambda(permute_tensor),  # Custom permutation\n        transforms.ConvertImageDtype(torch.float16),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Process frames in chunks\n    frame_dict = {}\n    for start_frame in tqdm(range(0, total_frames, chunk_size), desc=\"Processing chunks\"):\n        end_frame = min(start_frame + chunk_size, total_frames)\n        video_chunk, _, _ = read_video(video_path, start_pts=pts[start_frame], end_pts=pts[end_frame-1], pts_unit='sec')\n        \n        for i in range(0, video_chunk.shape[0], frame_rate):\n            frame_index = start_frame + i\n            if frame_index >= total_frames:\n                break\n            frame_id = f\"{os.path.basename(video_path).split('.')[0]}_frame_{frame_index}\"\n            \n            # Debug print\n            # print(f\"Frame shape before transform: {video_chunk[i].shape}\")\n            \n            frame_tensor = transform(video_chunk[i])\n            \n            # Move the tensor to the selected device (CPU or CUDA)\n            frame_tensor = frame_tensor.to(device)\n            \n            # Debug print\n            # print(f\"Frame shape after transform: {frame_tensor.shape} on device: {device}\")\n            \n            frame_dict[frame_id] = frame_tensor\n        \n        # Clear memory\n        del video_chunk\n        torch.cuda.empty_cache()\n    \n    # Save the dictionary\n    tensor_path = os.path.join(tensor_output_folder, f\"{os.path.basename(video_path).split('.')[0]}_frames.pt\")\n    torch.save(frame_dict, tensor_path)\n    print(f\"Processed {len(frame_dict)} frames from {video_path}\")\n    \n    # Print total number of frames generated\n    print(f\"Total frames generated: {len(frame_dict)}\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_preprocess_single_video(video_path, tensor_output_folder, frame_rate=60):\n    \"\"\"\n    Test the preprocessing of a single video.\n    Args:\n    - video_path (str): Path to the video file.\n    - tensor_output_folder (str): Folder to save the tensor files.\n    - frame_rate (int): Interval of frames to be extracted (e.g., every 60th frame).\n    \"\"\"\n    extract_and_store_frames(video_path, tensor_output_folder, frame_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"output\", exist_ok = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preprocess_single_video('/kaggle/input/sample-bengaluru-mobility/18th_Crs_BsStp_JN_FIX_1_000.mp4', 'output', frame_rate=60)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## YOLOv8","metadata":{}},{"cell_type":"markdown","source":"### Reading tensors","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndef read_frames_from_file(file_path):\n    frame_dict = {}\n    \n    with open(file_path, 'rb') as f:\n        while True:\n            frame_id_length_bytes = f.read(4)\n            if not frame_id_length_bytes:\n                break\n            \n            frame_id_length = int.from_bytes(frame_id_length_bytes, byteorder='big')\n            frame_id = f.read(frame_id_length).decode('utf-8')\n            \n            tensor_size_bytes = f.read(4)\n            tensor_size = int.from_bytes(tensor_size_bytes, byteorder='big')\n            \n            tensor_data = f.read(tensor_size)\n            tensor_array = np.frombuffer(tensor_data, dtype=np.uint8).reshape((3, 90, 160))  # Change shape as necessary\n            \n            tensor = torch.tensor(tensor_array, dtype=torch.float32) / 255.0\n            frame_dict[frame_id] = tensor\n    \n    return frame_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fdict = read_frames_from_file(\"/kaggle/working/output/18th_Crs_BsStp_JN_FIX_1_000_frames.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fdict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ultranalytics","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:52:45.903538Z","iopub.execute_input":"2024-07-30T18:52:45.903866Z","iopub.status.idle":"2024-07-30T18:52:50.551984Z","shell.execute_reply.started":"2024-07-30T18:52:45.903841Z","shell.execute_reply":"2024-07-30T18:52:50.550800Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nimport os\n\n# Define the preprocessing transformation used during saving\npreprocess = transforms.Compose([\n    transforms.Resize((640, 640)),  # Ensure this matches the preprocessing used during saving\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndef tensor_to_pil_image(tensor):\n    \"\"\"\n    Convert a tensor to a PIL Image.\n    \n    Args:\n    - tensor (torch.Tensor): Tensor to convert.\n    \n    Returns:\n    - PIL.Image.Image: Converted PIL Image.\n    \"\"\"\n    # Remove batch dimension and convert to CPU\n    tensor = tensor.squeeze().cpu()\n    # Convert tensor to PIL image\n    pil_image = transforms.ToPILImage()(tensor)\n    return pil_image\n\ndef load_tensors(tensor_file_path):\n    \"\"\"\n    Load tensors from a file and return them as a dictionary.\n    \n    Args:\n    - tensor_file_path (str): Path to the file containing the saved tensors.\n    \n    Returns:\n    - dict: Dictionary with frame IDs as keys and tensors as values.\n    \"\"\"\n    if not os.path.isfile(tensor_file_path):\n        raise FileNotFoundError(f\"The file {tensor_file_path} does not exist.\")\n    \n    # Load the dictionary of tensors from the file\n    tensor_dict = torch.load(tensor_file_path, map_location='cpu')\n    \n    return tensor_dict\n\ndef segment_and_visualize_frames(tensor_dict):\n    \"\"\"\n    Run YOLO model on frames and visualize the results.\n    \n    Args:\n    - tensor_dict (dict): Dictionary with frame IDs as keys and tensors as values.\n    \"\"\"\n    model = YOLO('yolov8s.pt')  # Load the YOLO model\n    \n    for frame_id, frame_tensor in tensor_dict.items():\n        # Convert tensor to PIL image\n        frame_pil = tensor_to_pil_image(frame_tensor)\n        \n        # Run YOLO model on the image\n        results = model(frame_pil)\n        \n        # Visualize results\n        result_image = results[0].plot()\n        \n        # Display image\n        plt.imshow(result_image)\n        plt.title(f\"Segmented Frame ID: {frame_id}\")\n        plt.axis('off')\n        plt.show()\n\n# Example usage\ntensor_file_path = 'path/to/your/tensors.pt'\ntensor_dict = load_tensors(tensor_file_path)\nsegment_and_visualize_frames(tensor_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segment_and_visualize_frames(fdict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attempt 2","metadata":{}},{"cell_type":"code","source":"def process_and_save_video(video_path, output_path):\n    # Open the video file\n    video_reader = imageio.get_reader(video_path)\n    \n    # Get total number of frames\n    num_frames = video_reader.count_frames()\n\n    tensor_list = []\n    frame_count = 0\n    frame_dict = {}\n    \n    # Use tqdm for progress bar\n    for frame in tqdm(video_reader, total=num_frames, desc=\"Processing Frames\"):\n        # Convert the frame to a PIL Image\n        pil_image = Image.fromarray(frame)\n        \n        # Preprocess the frame\n        frame_tensor = preprocess(pil_image)\n        frame_tensor = frame_tensor.unsqueeze(0)  # Add batch dimension\n\n        tensor_list.append(frame_tensor)\n        frame_dict[frame_count] = frame_tensor\n        frame_count += 1\n\n    # Concatenate tensors along the batch dimension\n    all_tensors = torch.cat(tensor_list, dim=0)\n\n    # Save the tensor to disk\n    torch.save(all_tensors, output_path)\n    print(f\"Saved {frame_count} frames to {output_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:53:24.231050Z","iopub.execute_input":"2024-07-30T18:53:24.231399Z","iopub.status.idle":"2024-07-30T18:53:24.237052Z","shell.execute_reply.started":"2024-07-30T18:53:24.231372Z","shell.execute_reply":"2024-07-30T18:53:24.236395Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"process_and_save_video('/kaggle/input/sample-bengaluru-mobility/18th_Crs_BsStp_JN_FIX_1_000.mp4', 'output_tensors.pt')","metadata":{"execution":{"iopub.status.busy":"2024-07-30T18:53:25.784124Z","iopub.execute_input":"2024-07-30T18:53:25.785032Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Processing Frames: 100%|██████████| 22500/22500 [10:18<00:00, 36.37it/s]\n","output_type":"stream"}]}]}